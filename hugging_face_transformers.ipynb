{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0AIAEN102-2022-01-01\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> **Introduction to Natural Language Processing  with Hugging Face Transformers** <center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0AIAEN/Transformers_models.png\" width=\"60%\" alt=\"iris image\"> <center>\n",
    "\n",
    "Image Source: [nvidia](https://blogs.nvidia.com/blog/2022/03/25/what-is-a-transformer-model/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0AIAEN102-2022-01-01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Guided Project will walk you through some of the applications of Hugging Face Transformers in Natural Language Processing (NLP).\n",
    "\n",
    "Hugging Face Transformers package is very popular and versatile Python library that provides pre-trained models for a variety of applications in NLP, as well other areas such as image analysis, audio analysis, multimodal analysis (optical character recognition, video classification, visual question answering and many more). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Guided Project will focus on text analysis tasks, which are:\n",
    "\n",
    "*   Text Classification.\n",
    "    *   Sentiment Analysis. Classifies the polarity of a given text.\n",
    "*   Topic Classification. Classifies sequences into specified class names.\n",
    "*   Text Generator. Generates text from a given input.\n",
    "*   Token Classification.\n",
    "    *   Name Entity Recognition (NER). Labels each word with the entity it represents.\n",
    "*   Question answering. Extracts the answer from the context.\n",
    "*   Text Summarization. Generates a summary of a long sequence of text or document.\n",
    "*   Translation. Translates text into another language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n",
    "            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Background-(optional)\">Background (optional)</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Example-1--Sentiment-Analysis?\">Example 1 - Sentiment Analysis</a></li>\n",
    "            <li><a href=\"#Example-2--Topic-Classification?\">Example 2 - Topic Classification</a></li>\n",
    "            <li><a href=\"#Example-3--Text-Generator?\">Example 3 - Text Generator: Masked Language Modeling</a></li>\n",
    "            <li><a href=\"#Example-4--Name-Entity-Recognition?\">Example 4 - Name Entity Recognition</a></li>\n",
    "            <li><a href=\"#Example-5--Question-Answering?\">Example 5 - Question Answering</a></li>\n",
    "            <li><a href=\"#Example-6--Text-Summarization?\">Example 6 - Text Summarization</a></li>\n",
    "            <li><a href=\"#Example-7--Translation?\">Example 7 - Translation</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "</ol>\n",
    "\n",
    "<a href=\"#Exercises\">Exercises</a>\n",
    "<ol>\n",
    "    <li><a href=\"#Exercise-1---Sentiment-Analysis)\">Exercise 1. Sentiment Analysis</a></li>\n",
    "    <li><a href=\"#Exercise-2---Topic-Classification\">Exercise 2. Topic Classification</a></li>\n",
    "    <li><a href=\"#Exercise-3---Text-Generation\">Exercise 3. Text Generation</a></li>\n",
    "    <li><a href=\"#Exercise-4---Name-Entity-Recognition\">Exercise 4. Name Entity Recognition</a></li>\n",
    "    <li><a href=\"#Exercise-5---Question-Answering\">Exercise 5. Question Answering</a></li>\n",
    "    <li><a href=\"#Exercise-6---Text-Summarization\">Exercise 6. Text Summarization</a></li>\n",
    "    <li><a href=\"#Exercise-7---Translation\">Exercise 7. Translation</a></li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    " - Use Hugging Face Transformers to do:\n",
    "   - Sentiment Analysis\n",
    "   - Topic Classification\n",
    "   - Text Generation\n",
    "   - Name Entity Recognition\n",
    "   - Question Answering\n",
    "   - Text Summarization\n",
    "   - Text Translation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we will be using the following libraries:\n",
    "\n",
    "*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for managing the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Required Libraries\n",
    "\n",
    "The following required libraries are pre-installed in the Skills Network Labs environment. However, if you run this notebook commands in a different Jupyter environment (e.g. Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!mamba` in the code cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n",
    "# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n",
    "# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (1.13.1+cpu)\n",
      "Requirement already satisfied: typing-extensions in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from torch) (4.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (1.13.1+cpu)\n",
      "Requirement already satisfied: typing-extensions in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from torch) (4.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.13.2-py3-none-any.whl (512 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.7/512.7 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting evaluate\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: transformers[sentencepiece] in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (4.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from datasets) (1.21.6)\n",
      "Collecting pyarrow>=8.0.0 (from datasets)\n",
      "  Downloading pyarrow-12.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.1/39.1 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.7,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from datasets) (2.29.0)\n",
      "Collecting tqdm>=4.62.1 (from datasets)\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/194.6 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.15-py37-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from datasets) (2023.1.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.8.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (987 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m988.0/988.0 kB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from datasets) (0.16.4)\n",
      "Requirement already satisfied: packaging in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from datasets) (4.11.4)\n",
      "Collecting responses<0.19 (from evaluate)\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: filelock in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from transformers[sentencepiece]) (3.12.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from transformers[sentencepiece]) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from transformers[sentencepiece]) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from transformers[sentencepiece]) (0.4.2)\n",
      "Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece])\n",
      "  Downloading sentencepiece-0.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting protobuf<=3.20.3 (from transformers[sentencepiece])\n",
      "  Downloading protobuf-3.20.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.8/99.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (289 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.8/289.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.3.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (148 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.0/148.0 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting asynctest==0.13.0 (from aiohttp->datasets)\n",
      "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from aiohttp->datasets) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.15.0)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.7/115.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: sentencepiece, xxhash, tqdm, pyarrow, protobuf, multidict, frozenlist, dill, asynctest, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets, evaluate\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.60.0\n",
      "    Uninstalling tqdm-4.60.0:\n",
      "      Successfully uninstalled tqdm-4.60.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.21.8\n",
      "    Uninstalling protobuf-4.21.8:\n",
      "      Successfully uninstalled protobuf-4.21.8\n",
      "Successfully installed aiohttp-3.8.6 aiosignal-1.3.1 async-timeout-4.0.3 asynctest-0.13.0 datasets-2.13.2 dill-0.3.6 evaluate-0.4.1 frozenlist-1.3.3 multidict-6.0.5 multiprocess-0.70.14 protobuf-3.20.3 pyarrow-12.0.1 responses-0.18.0 sentencepiece-0.2.0 tqdm-4.66.2 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets evaluate transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: regex in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from sacremoses) (2023.12.25)\n",
      "Requirement already satisfied: six in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from sacremoses) (1.16.0)\n",
      "Requirement already satisfied: click in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from sacremoses) (8.1.3)\n",
      "Collecting joblib (from sacremoses)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from sacremoses) (4.66.2)\n",
      "Requirement already satisfied: importlib-metadata in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from click->sacremoses) (4.11.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses) (3.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses) (4.5.0)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=7856865a105ebcb4165ce0f1246625a12c6b32162896603672da64db5b5ff60d\n",
      "  Stored in directory: /home/jupyterlab/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: joblib, sacremoses\n",
      "Successfully installed joblib-1.3.2 sacremoses-0.0.53\n"
     ]
    }
   ],
   "source": [
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please **restart the kernel** after running the above installs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "\n",
    "_We recommend you import all required libraries in one place (here):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from scikit-learn) (1.21.6)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from scikit-learn) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "!pip install -U scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background (optional)\n",
    "\n",
    "### What is a Transformer Model?\n",
    "\n",
    "A Transformer Model is a neural network that learns context and thus meaning by tracking relationships in sequential data, for example, words in a sentence. Transformer models apply an evolving set of mathematical techniques, called attention or self-attention, differently weighing the significance of each part of the input data. They are used primarily in the fields of natural language processing and computer vision. Similarly to recurrent neural networks (RNNs), transformers are designed to process sequential data, for example a body of text, with applications such as translation and text summarization. However, unlike RNNs, transformers process the entire input all at once. The attention mechanism provides context for any position in the input sequence. For example, if the input data is a natural language sentence, the transformer does not have to process one word at a time. This allows for more parallelization than RNNs, and therefore, reduces training times. The additional training parallelization allows training on larger datasets. This led to the development of pre-trained systems such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), which were trained with large language datasets, such as the Wikipedia Corpus and Common Crawl, and can be fine-tuned for specific tasks.\n",
    "\n",
    "Transformer models were first described in a [2017 paper](https://arxiv.org/abs/1706.03762?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0AIAEN102-2022-01-01) from Google, as the newest and the most powerful classes of models invented today, replacing some RNNs.\n",
    "\n",
    "### What do Transformer Models do?\n",
    "\n",
    "Transformer Models have many useful applications in today world. They are widely used in near real-time translation tasks, opening communication spaces to language-diverse and hearing-impaired audiences. These models are advancing medical research by helping scientists to understand the DNA and amino acid sequences to speed up the development and improve the quality of treatments for different diseases. The models can detect anomalies to prevent fraud detection, streamline manufacturing, make recommendations and overall improve our day-to-day quality of life.\n",
    "\n",
    "### Transformer Model Architecture\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0AIAEN/The-Transformer-model-architecture.png\" width=\"50%\" alt=\"iris image\"> \n",
    "\n",
    "Image Source: [wikipedia](https://en.wikipedia.org/wiki/Transformer_\\(machine_learning_model\\)?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0AIAEN102-2022-01-01)\n",
    "\n",
    "The diagram above describes the pipeline between the input, output and positional encoding. For more information on Transformer Architecture visit this [wikipedia](https://en.wikipedia.org/wiki/Transformer_\\(machine_learning_model\\)?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0AIAEN102-2022-01-01) page. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use Hugging Face Transformers\n",
    "\n",
    "There is very easy way to apply Hugging Face Transformers, it is to use the `pipeline()` function. The pipeline makes it simple to use any model from the Hugging Face Hub for inference on any language, computer vision, speech, or multimodal tasks. Each task has an associated pipeline. However, it is simpler to use the general pipeline abstraction, which contains all the task-specific pipelines. The `pipeline()` automatically loads a default model and a pre-processing class capable of inference for any of your desired tasks.\n",
    "\n",
    "The links beside each of the 7 Examples shown here, are from Hugging Face community and contain more information about the models and data they were trained on.\n",
    "This [Hugging Face Page](https://huggingface.co/models?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0AIAEN102-2022-01-01) also contains ALL the models created by this community.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1 - [Sentiment Analysis](https://huggingface.co/blog/sentiment-analysis-python?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0AIAEN102-2022-01-01) \n",
    "\n",
    "\n",
    "**Sentiment analysis** is a natural language processing technique that identifies the polarity of a given text. Some of the most common practical uses of sentiment analysis are tweets analysis, product reviews, support tickets classification and others. Sentiment analysis allows quick processing of large amounts, real-time data. The Diagram below, describes the process of Sentiment Analysis. As shown in the Diagram below, we can use sentiment analysis to predict a label and a score of a random amazon product review. \n",
    "\n",
    "<center> <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0AIAEN/sent_analysis6.png\" width=\"70%\"> <center>\n",
    "\n",
    "The evaluation metric for this type of analysis is accuracy score. The default model behind \"sentiment-analysis\" pipeline is \"distilbert-base-uncased-finetuned-sst-2-english\" model, pre-trained on a large corpus of English data in a self-supervised fashion. This means it was pre-trained on the raw texts only, with no humans labelling them in any way. For more information on this model read [here.](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0AIAEN102-2022-01-01&text=I+like+you.+I+love+you)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by loading text classification pipeline from `pipeline()` using \"sentiment-analysis\" task identifier. It uses the default, \"distilbert-base-uncased-finetuned-sst-2-english\" model for sentiment analysis. Note, you can skip specifying a default model, but you will recieve a warning message.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (4.30.2)\n",
      "Requirement already satisfied: filelock in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from transformers) (2.29.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: importlib-metadata in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from transformers) (4.11.4)\n",
      "Requirement already satisfied: fsspec in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests->transformers) (2023.5.7)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can copy a random product review from amazon and input it into selected classifier. [Review Source.](https://www.amazon.ca/iRobot-Roomba-Self-Emptying-Robot-Vacuum/dp/B094NYHTMF/ref=sr_1_1_sspa?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0AIAEN102-2022-01-01&crid=2LO2ZA75UKXKQ&keywords=roomba&qid=1663097073&sprefix=roomba%2Caps%2C194&sr=8-1-spons&psc=1#customerReviews) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the sentiment is classified as Positive with 99.8% accuracy score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998728036880493}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "Classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Now you can use the Classifier to analyze sentiment:\n",
    "sentiment = Classifier(\"This is a positive review!\")\n",
    "print(sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2 - [Topic Classification](https://huggingface.co/facebook/bart-large-mnli?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0AIAEN102-2022-01-01)\n",
    "\n",
    "\n",
    "**Topic Classification** task classifies sequences into specified class names. It applies \"zero-shot-classification\" algorithm to perform this task. Zero-Shot Learning (ZSL) is when a classifier learns on one set of labels and then evaluates on a different set of labels, the ones that it has never seen before.  [This link](https://joeddav.github.io/blog/2020/05/29/ZSL.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0AIAEN102-2022-01-01) has more information about ZSL. \n",
    "As shown in the Diagram below, we also need to specify some kind of descriptor (input labels) for an unseen class (such as a set of visual attributes or simply the class name) in order for our model to be able to predict that class without training data.\n",
    "\n",
    "<center> <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0AIAEN/sent_analysis5.png\" width=\"70%\"> <center>\n",
    "\n",
    "First, we load a pipeline with \"zero-shot-classification\", pass a sequence that we want to classify and a list of candidate labels and see how the model will assign corresponding labels to the input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'Exploratory Data Analysis is the first course in Machine Learning Program that introduces learners to the broad range of Machine Learning concepts, applications, challenges, and solutions, while utilizing interesting real-life datasets.', 'labels': ['data analysis', 'art', 'natural science'], 'scores': [0.995668351650238, 0.0026282661128789186, 0.0017033647745847702]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "text = \"Exploratory Data Analysis is the first course in Machine Learning Program that introduces learners to the broad range of Machine Learning concepts, applications, challenges, and solutions, while utilizing interesting real-life datasets.\"\n",
    "candidate_labels = [\"art\", \"natural science\", \"data analysis\"]\n",
    "\n",
    "predictions = classifier(\n",
    "    text, candidate_labels=candidate_labels\n",
    ")\n",
    "\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, 'data analysis' is the most successful candidate for the topic of this input, having 99.6% score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3 - [Text Generator](https://huggingface.co/tasks/text-generation?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0AIAEN102-2022-01-01)\n",
    "\n",
    "\n",
    "**Text Generation** model is also known as causal language model, is a task of predicting a next word in a sentence, given some previous input. This task is very similar to the auto-correct function we have on our phones. Classification metric cannot be used in this task, as there is no single correct answer. Instead, text distribution auto-completed by the model is evaluated by the cross entropy loss and perplexity. The default model behind Text Generation is Generative Pre-trained Transformer 2, [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0AIAEN102-2022-01-01) model. It can receive an input like \"This course will teach you\" and proceed to complete the sentence based on those first words, as shown in the Diagram below. \n",
    "\n",
    "<center> <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0AIAEN/text_gener2.png\" width=\"60%\" alt=\"iris image\"> <center>\n",
    "\n",
    "Similarly to Completion Generation Models, we also have Text-to-Text Models. These models are trained to learn the mapping between a pair of texts (e.g. translation from one language to another). The most popular variants of these models are T5, T0 and BART. Text-to-Text models are trained with multi-tasking capabilities, they can accomplish a wide range of tasks, including summarization, translation, and text classification. \n",
    "\n",
    "Interestingly, causal language models can also be used to generate a code to help programmers in their repetitive coding tasks.\n",
    "\n",
    "To begin our analysis, we can load a pipeline with the default \"text-generation\" model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'This course will teach you how to make and use a Raspberry Pi keyboard, using the built-in serial port.\\n\\nThe project name comes from the idea of bringing Raspberry Pi keyboards to the next level.\\n\\nThis project is dedicated to helping'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model = \"gpt2\"  # Choose your desired model\n",
    "generator = pipeline(\"text-generation\", model=model)\n",
    "\n",
    "prompt = \"This course will teach you how to make and use a Raspberry Pi keyboard, using the built-in serial port.\"\n",
    "\n",
    "generated_text = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can also use \"distilgpt2\" model, as well as some parameters, such length and number of the sentences needed. Distilled GPT-2 model is an English-language model pre-trained with the supervision of the smallest version of GPT-2. Like GPT-2, DistilGPT2 can be used to generate text. For more information about this model, please visit this [link](https://huggingface.co/distilgpt2?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0AIAEN102-2022-01-01).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'This course will teach you the key concepts of evolution. Here are the two parts!'},\n",
       " {'generated_text': 'This course will teach you much more about the fundamentals of language development. And how to build what makes this course so useful, how it should be written'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\n",
    "    \"This course will teach you\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masked Language Modeling\n",
    " \n",
    "\n",
    "Sometimes, it is useful to use Masked Language modeling, which also has Text Generation capabilities. **Masked language** modeling is the task of masking some of the words in a sentence and predicting which words should replace those masks. These models are useful when we want to get a statistical understanding of the language in which the model is trained in. Masked language models do not require labelled data! They are trained by masking a couple of words in sentences and the model is expected to guess the masked word. The Diagram below shows a simple representation of this concept.\n",
    "\n",
    "<center> <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0AIAEN/fill_mask3.png\" width=\"60%\" alt=\"iris image\"> <center>\n",
    "\n",
    "For example, masked language modeling is used to train large models for domain-specific problems. If you have to work on a domain-specific task, such as retrieving information from medical research papers, you can train a masked language model using those papers.\n",
    "\n",
    "For more information about \"fill-mask\" model you can read [here.](https://huggingface.co/tasks/fill-mask?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0AIAEN102-2022-01-01)\n",
    "The Example below, shows a few options for a 'masked' word in the input sentence. Let's see 4 options in the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c4bf37aab6410b80f0d52d4ab5e88a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9cc6e605a74a74adae89a477bfb46e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab50fcfc140d48fb97cb29546288e9d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aea07ac34b84fc9baf525bbebe2c790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18267f7b63a14205be4ddb09ff1267a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab19360a0a094358b523a96c398a6b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.196197971701622,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical',\n",
       "  'sequence': 'This course will teach you all about mathematical models.'},\n",
       " {'score': 0.04052729904651642,\n",
       "  'token': 38163,\n",
       "  'token_str': ' computational',\n",
       "  'sequence': 'This course will teach you all about computational models.'},\n",
       " {'score': 0.03301800787448883,\n",
       "  'token': 27930,\n",
       "  'token_str': ' predictive',\n",
       "  'sequence': 'This course will teach you all about predictive models.'},\n",
       " {'score': 0.03194150701165199,\n",
       "  'token': 745,\n",
       "  'token_str': ' building',\n",
       "  'sequence': 'This course will teach you all about building models.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\", \"distilroberta-base\")\n",
    "unmasker(\"This course will teach you all about <mask> models.\", top_k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4 - [Name Entity Recognition (NER)](https://huggingface.co/Jean-Baptiste/camembert-ner?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0AIAEN102-2022-01-01)\n",
    "\n",
    "\n",
    "**NER** sometimes also referred as entity chunking, extraction, or identification, is the task of identifying and categorizing key information (entities) in text. The model sorts according to name of the person: 'PER', group: 'ORG', and location: 'LOC' with appropriate accuracy score and token location. \n",
    "\n",
    "The default model behind NER is \"camembert-ner\". It was trained and fine tuned on wikiner-fr dataset (~170 634 sentences). Click [here](https://medium.com/mysuperai/what-is-named-entity-recognition-ner-and-how-can-i-use-it-2b68cf6f545d?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0AIAEN102-2022-01-01) to learn more about NER.\n",
    "\n",
    "<center> <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0AIAEN/ner2.png\" width=\"60%\" alt=\"iris image\"> <center> \n",
    "\n",
    "Let's look at the specific Example, where we load a pipeline with \"ner\" model and some input:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  \"`grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9993105,\n",
       "  'word': 'Roberta',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9976597,\n",
       "  'word': 'IBM Skills Network',\n",
       "  'start': 35,\n",
       "  'end': 53},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.99702173,\n",
       "  'word': 'Toronto',\n",
       "  'start': 57,\n",
       "  'end': 64}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", grouped_entities=True)\n",
    "ner(\"My name is Roberta and I work with IBM Skills Network in Toronto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9993105, 'word': 'Roberta', 'start': 11, 'end': 18}, {'entity_group': 'ORG', 'score': 0.9976597, 'word': 'IBM Skills Network', 'start': 35, 'end': 53}, {'entity_group': 'LOC', 'score': 0.99702173, 'word': 'Toronto', 'start': 57, 'end': 64}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Definisikan ner\n",
    "ner = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", grouped_entities=True)\n",
    "print(ner(\"My name is Roberta and I work with IBM Skills Network in Toronto\"))\n",
    "\n",
    "# Kemudian Anda bisa menghapusnya\n",
    "del ner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the model properly identifies all entities in the sentence with highest confidence scores. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5 - [Question Answering](https://huggingface.co/tasks/question-answering?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0AIAEN102-2022-01-01)\n",
    "\n",
    "\n",
    "Another widely used application of Hugging Face transformers is Question Answering task. **Question Answering** is the task of extracting an answer from a document. QA models take in a context parameter, which is a document in which you are searching for some information, and a question, and return an answer. The answer is being extracted, not generated. The task is evaluated on two metrics: exact match and F1-score.\n",
    "As discussed in Example 3, there are also other QA models that are not extractive but generative. They generate free text directly based on the context, as shown in the Diagram below.\n",
    "\n",
    "<center> <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0AIAEN/q_a1.png\" width=\"60%\"> <center> \n",
    "\n",
    "Let's look at the Example where we have some context and we try to extract some information from it.\n",
    "First, we load the `pipeline()` with \"question-answering\" identifier. We also input our question and content. Then we apply our model to the input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e570b9e78fd42f1baddab371f9f814d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d9a1f0a1d54cbc8f609aac1cdad629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17cf73c9b28e4fcfa453a5bc7855b2a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba0398179244ec5a15fd9671557d06a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff16f039e17d431abb7b0d755a8ae26e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.8247062563896179, 'start': 48, 'end': 56, 'answer': 'Amazonia'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_model = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
    "question = \"Which name is also used to describe the Amazon rainforest in English?\"\n",
    "context = \"The Amazon rainforest, also known in English as Amazonia or the Amazon Jungle.\"\n",
    "qa_model(question = question, context = context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the correct answer has been extracted with 82% confidence score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 6: [Text Summarization](https://huggingface.co/tasks/summarization?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0AIAEN102-2022-01-01)\n",
    "\n",
    "\n",
    "The next Example of this Guided Project deals with Text Summarization. **Text Summarization** is the task of creating a shorter version of a document, while preserving the relevant information and importance of the original document. The summarizer model takes in the whole document as input and outputs the summarized version. The evaluation metric used in this analysis is called Rouge. It is a benchmark based on the shared sabsequent tokens between the produced sequence and the original document.\n",
    "\n",
    "<center> <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0AIAEN/summarization.png\" width=\"60%\"> <center> \n",
    "\n",
    "Let's load the \"summarization\" pipeline, input some text that we want to summarize, and see what the output will look like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3fc7b36ca14763948548d7e0e52da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce8807acf4544a27b1b6c637bc11c28e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10354e3d8b6246f88f3672cc34dd854f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212ff791ac8141afbe2e86a7dc4ee407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da5272bab4d44c7689a6b7e0a468c33c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' Exploratory Data Analysis is the first course in Machine Learning Program that introduces learners to the broad range of Machine Learning concepts, applications, challenges, and solutions . EDA is a visual and statistical process that allows us to take a glimpse into the data before the analysis . It lays foundation for the analysis so our results go along with our expectations .'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "Exploratory Data Analysis is the first course in Machine Learning Program that introduces learners to the broad range of Machine Learning concepts, applications, challenges, and solutions, while utilizing interesting real-life datasets. So, what is EDA and why is it important to perform it before we dive into any analysis?\n",
    "EDA is a visual and statistical process that allows us to take a glimpse into the data before the analysis. It lets us test the assumptions that we might have about the data, proving or disproving our prior believes and biases. It lays foundation for the analysis, so our results go along with our expectations. In a way, it’s a quality check for our predictions.\n",
    "As any data scientist would agree, the most challenging part in any data analysis is to obtain a good quality data to work with. Nothing is served to us on a silver plate, data comes in different shapes and formats. It can be structured and unstructured, it may contain errors or be biased, it may have missing fields, it can have different formats than what an untrained eye would perceive. For example, when we import some data, very often it would contain a time stamp. To a human it is understandable format that can interpreted. But to a machine, it is not interpretable, so it needs to be told what that means, the data needs to be transformed into simple numbers first. There are also different date-time conventions depending on a country (i.e., Canadian versus USA), metric versus imperial systems, and many other data features that need to be recognized before we start doing the analysis. Therefore, the first step before performing any analysis – is get really aquatinted with your data!\n",
    "This course will teach you to ‘see’ and to ‘feel’ the data as well as to transform it into analysis-ready format. It is introductory level course, so no prior knowledge is required, and it is a good starting point if you are interested in getting into the world of Machine Learning. The only thing that is needed is some computer with internet, your curiosity and eagerness to learn and to apply acquired knowledge.  If you live in Canada, you might be interested about gasoline prices in different cities or if you are an insurance actuary you need to analyze the financial risks that you will take based on your clients information. Whatever is the case, you will be able to do your own analysis, and confirm or disprove some of the existing information.\n",
    "The course contains videos and reading materials, as well as well as a lot of interactive practice labs that learners can explore and apply the skills learned. It will allow you to use Python language in Jupyter Notebook, a cloud-based skills network environment that is pre-set for you with all available to be downloaded packages and libraries. It will introduce you to the most common visualization libraries such as Pandas, Seaborn, and Matplotlib to demonstrate various EDA techniques with some real-life datasets.\n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a short summary of our paragraph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 7 - [Translation](https://huggingface.co/course/chapter7/4?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0AIAEN102-2022-01-01&fw=pt)\n",
    "\n",
    "The last Example of this Guided Project shows the Translation application. **Translation** models take an input in some source language and output the translation in a target language. The evaluation metric used for this task is called BLEU (bilingual evaluation understudy). It is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. It is on a scale from 0 to 1, 1 meaning perfect score.\n",
    "\n",
    "The are two types of models, *monolingual* models, trained on a specific language duo data, and there are *multilingual* models, trained on multiple languages dataset. \n",
    "\n",
    "\n",
    "<center> <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0AIAEN/translation.png\" width=\"70%\"> <center> \n",
    "\n",
    "The Example below shows a monolingual, French-English model, \"translation_en_to_fr\", which levereges T5-base model under the hood. These models add a task prefix, e.g., \"_en_to_fr\", indicating the task itself, such as translate English to French. There are also multilingual models for inference. Their inference usage differs from monolingual models. For more information on how to use multilingual models, please visit this [documentation](https://huggingface.co/transformers/v3.0.2/multilingual.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMGPXX0AIAEN102-2022-01-01).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock (from transformers)\n",
      "  Downloading filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.12.25-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (761 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m761.6/761.6 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from transformers) (2.29.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from transformers) (4.60.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from transformers) (4.11.4)\n",
      "Requirement already satisfied: fsspec in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests->transformers) (2023.5.7)\n",
      "Installing collected packages: tokenizers, safetensors, regex, filelock, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.12.2 huggingface-hub-0.16.4 regex-2023.12.25 safetensors-0.4.2 tokenizers-0.13.3 transformers-4.30.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to use a specific model that is from one specific language to another, you can also directly use the translation pipeline without specifying the model under the hood. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ca207606bf48598213a06ea34ddbf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e5156b6afd422ca6ab4251064b3699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac5d265563240e1b57671406fbfe328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e038cf6ab84cd6b1a2d289b0c809a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65cfa836bf1a4642b0c75fa67a50ae13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c6de1d0dba4a28a4bea648cb302547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'Quel est votre âge ?'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Membuat translator dari bahasa Inggris ke Prancis\n",
    "en_fr_translator = pipeline(\"translation_en_to_fr\", model=\"t5-small\")\n",
    "\n",
    "# Menerjemahkan teks dari bahasa Inggris ke Prancis\n",
    "result = en_fr_translator(\"How old are you?\")\n",
    "\n",
    "# Menampilkan hasil terjemahan\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Let's Practice**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Sentiment Analysis\n",
    "For sentiment analysis, we can also use a specific model that is better suited to our use case by providing the name of the model. For example, if we want a sentiment analysis model for tweets, we can specify the following model id: \"cardiffnlp/twitter-roberta-base-sentiment\". This model has been trained on  ~58M tweets and fine-tuned for sentiment analysis with the \"TweetEval\" benchmark. \n",
    "The output labels for this model are: 0 -> Negative; 1 -> Neutral; 2 -> Positive.\n",
    "\n",
    "In this Exercise, use \"cardiffnlp/twitter-roberta-base-sentiment\" model pre-trained on tweets data, to analyze any tweet of choice. Optionally, use the default model (used in Example 1) on the same tweet, to see if the result will change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf5fa75dfbfe434eb9008bd7b9e62c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/747 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808fa122957848e38ab3d039c298955e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122a8fe423b8404b9362f15743abd68d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6356d66eb0ae4e2e90d2670f99bb9b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521909268e2f4a64b7d014756159d4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil analisis sentimen dengan model khusus untuk tweet: [{'label': 'LABEL_1', 'score': 0.7391404509544373}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad80401077d48459bdbbb832835b0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbfed1c33b044a669738a56a4f5a696a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc93b43392b4681b5b97fd6d686c4d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9d2fc8d5e2048348fe0de853be330f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil analisis sentimen dengan model umum: [{'label': 'NEGATIVE', 'score': 0.9913738369941711}]\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "from transformers import pipeline\n",
    "\n",
    "# Model khusus untuk analisis sentimen pada tweet\n",
    "specific_model = pipeline(model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "# Contoh tweet yang akan dianalisis\n",
    "tweet = \"Kecerdasan buatan dan otomatisasi sudah menyebabkan friksi di tempat kerja. Haruskah sekolah mengubah program-program yang ada untuk topik-topik seperti #AI, atau apakah diperlukan bidang penelitian baru?\"\n",
    "\n",
    "# Menggunakan model khusus untuk menganalisis sentimen tweet\n",
    "specific_result = specific_model(tweet)\n",
    "print(\"Hasil analisis sentimen dengan model khusus untuk tweet:\", specific_result)\n",
    "\n",
    "# Model umum untuk analisis sentimen\n",
    "general_model = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Menggunakan model umum untuk menganalisis sentimen tweet yang sama\n",
    "general_result = general_model(tweet)\n",
    "print(\"Hasil analisis sentimen dengan model umum:\", general_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting xformers\n",
      "  Downloading xformers-0.0.24.tar.gz (3.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of xformers to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading xformers-0.0.23.post1.tar.gz (3.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.12 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from xformers) (1.13.1+cpu)\n",
      "Requirement already satisfied: numpy in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from xformers) (1.21.6)\n",
      "Requirement already satisfied: typing-extensions in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from torch>=1.12->xformers) (4.5.0)\n",
      "Building wheels for collected packages: xformers\n",
      "  Building wheel for xformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for xformers: filename=xformers-0.0.23.post1-cp37-cp37m-linux_x86_64.whl size=579546 sha256=c33b0c8703a316dd71a1cf5ecf013d106ba4f9cc45900bbcf4f4ca9aecd3a952\n",
      "  Stored in directory: /home/jupyterlab/.cache/pip/wheels/f9/38/89/8b1fcad0cf46016b0c377b3b25fa3e4d8c7f68ccf6e42b263c\n",
      "Successfully built xformers\n",
      "Installing collected packages: xformers\n",
      "Successfully installed xformers-0.0.23.post1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil analisis sentimen dengan model khusus untuk tweet: [{'label': 'LABEL_1', 'score': 0.7391404509544373}]\n",
      "Hasil analisis sentimen dengan model umum: [{'label': 'NEGATIVE', 'score': 0.9913738369941711}]\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "from transformers import pipeline\n",
    "\n",
    "# Instal ulang Xformers\n",
    "!pip install xformers\n",
    "\n",
    "# Model khusus untuk analisis sentimen pada tweet\n",
    "specific_model = pipeline(model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "# Contoh tweet yang akan dianalisis\n",
    "tweet = \"Kecerdasan buatan dan otomatisasi sudah menyebabkan friksi di tempat kerja. Haruskah sekolah mengubah program-program yang ada untuk topik-topik seperti #AI, atau apakah diperlukan bidang penelitian baru?\"\n",
    "\n",
    "# Menggunakan model khusus untuk menganalisis sentimen tweet\n",
    "specific_result = specific_model(tweet)\n",
    "print(\"Hasil analisis sentimen dengan model khusus untuk tweet:\", specific_result)\n",
    "\n",
    "# Model umum untuk analisis sentimen\n",
    "general_model = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Menggunakan model umum untuk menganalisis sentimen tweet yang sama\n",
    "general_result = general_model(tweet)\n",
    "print(\"Hasil analisis sentimen dengan model umum:\", general_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "specific_model = pipeline(model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "data = \"Artificial intelligence and automation are already causing friction in the workforce. Should schools revamp existing programs for topics like #AI, or are new research areas required?\"\n",
    "specific_model(data)\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "original_model = pipeline(\"sentiment-analysis\")\n",
    "data = \"Artificial intelligence and automation are already causing friction in the workforce. Should schools revamp existing programs for topics like #AI, or are new research areas required?\"\n",
    "original_model(data)\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a Hint</summary>\n",
    "    \n",
    "As we see, tweet specific model classifies the tweet as neutral (LABEL_1), which it actually might be, versus the general model, classifies the same tweet as 'NEGATIVE', which is probably not entirely negative. \n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise 2 - Topic Classification\n",
    "In this Exercise, use any sentence of choice to classify it under any classes/ topics of choice. Use \"zero-shot-classification\" and specify the model=\"facebook/bart-large-mnli\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae604c478144f3b8aac323aa98b3899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2787e0939e49afb1d568b95ab17f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd98cd34aa4d4f39a9aa9f42e0ae9323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5a90efd13241bb89b16cff5973c5b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd3edea55ac41efb2f526cc35dea002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b2f5c100ce447d954037be5212f8a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kalimat: Saya sangat menyukai perjalanan dan belajar budaya baru\n",
      "Hasil klasifikasi:\n",
      "perjalanan: 0.7578278183937073\n",
      "pendidikan: 0.14914622902870178\n",
      "seni: 0.09302600473165512\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "from transformers import pipeline\n",
    "\n",
    "# Membuat pipeline untuk zero-shot classification dengan model \"facebook/bart-large-mnli\"\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Kalimat yang akan diklasifikasikan\n",
    "kalimat = \"Saya sangat menyukai perjalanan dan belajar budaya baru\"\n",
    "\n",
    "# Label-label kandidat yang mungkin\n",
    "label_kandidat = [\"seni\", \"pendidikan\", \"perjalanan\"]\n",
    "\n",
    "# Melakukan klasifikasi\n",
    "hasil_klasifikasi = classifier(\n",
    "    kalimat,\n",
    "    candidate_labels=label_kandidat,\n",
    ")\n",
    "\n",
    "# Menampilkan hasil klasifikasi\n",
    "print(\"Kalimat:\", kalimat)\n",
    "print(\"Hasil klasifikasi:\")\n",
    "for label, skor in zip(hasil_klasifikasi[\"labels\"], hasil_klasifikasi[\"scores\"]):\n",
    "    print(f\"{label}: {skor}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "classifier(\n",
    "    \"I love travelling and learning new cultures\",\n",
    "    candidate_labels=[\"art\", \"education\", \"travel\"],\n",
    ")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - Text Generation Models\n",
    "\n",
    "In this Exercise, use 'text-generator' and 'gpt2' model to complete any sentence. Define any desirable number of returned sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f37fcbd6bd4195baab21ce52ca6fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab9890cbc3643e496919771f5020143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58153873b03d48fe9a91f97b43621e43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dcbab95693448cba213495287e129eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be0018dd3b3474c9c2e61bced628b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6c434ea43f4ed896290033f18daf46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b7c74e81294f26b23582aa60132520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil lengkapan kalimat:\n",
      "Kalimat 1: Halo, saya adalah model bahasa\n",
      "\n",
      "\"This is what was found by them as part of the excavations so far,\" the researcher, Shlomran Shah, said, adding he would use the findings to build\n",
      "\n",
      "Kalimat 2: Halo, saya adalah model bahasa zidamah. So do you agree with him that we should keep praying for the prophet and you in every state as well of life?\n",
      "\n",
      "[I answer to, \"NO\n",
      "\n",
      "Kalimat 3: Halo, saya adalah model bahasa zak-iram – bahasa zak-irawahabahabahabahabahabahabahabatahabahab, zak shaw\n",
      "\n",
      "Kalimat 4: Halo, saya adalah model bahasa babba, \"I believe Allah has given me a better life than he has given me (in Paradise),\" says a young man named Muhtar of Lahore (Kot-z\n",
      "\n",
      "Kalimat 5: Halo, saya adalah model bahasa, Ibn al-Ismail\n",
      "\n",
      "And that's why we have no excuse for doing so, that will not help us.\n",
      "\n",
      "A young man called Mohammed bin Rifaat\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "from transformers import pipeline\n",
    "\n",
    "# Membuat pipeline untuk text generation dengan model \"gpt2\"\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "# Kalimat yang akan dilengkapi\n",
    "kalimat = \"Halo, saya adalah model bahasa\"\n",
    "\n",
    "# Menentukan jumlah kalimat yang ingin dihasilkan\n",
    "jumlah_kalimat = 5\n",
    "\n",
    "# Menyempurnakan kalimat dengan model gpt2\n",
    "hasil = generator(kalimat, max_length=50, num_return_sequences=jumlah_kalimat)\n",
    "\n",
    "# Menampilkan hasil lengkapan kalimat\n",
    "print(\"Hasil lengkapan kalimat:\")\n",
    "for i, hasil_kalimat in enumerate(hasil, start=1):\n",
    "    print(f\"Kalimat {i}: {hasil_kalimat['generated_text']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "generator = pipeline('text-generation', model = 'gpt2')\n",
    "generator(\"Hello, I'm a language model\", max_length = 30, num_return_sequences=3)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 - Name Entity Recognition\n",
    "In this Exercise, use any sentence of choice to extract entities: person, location and organization, using Name Entity Recognition task, specify model as \"Jean-Baptiste/camembert-ner\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628e729cb01f4eea83a56ec10927db9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499b801dde9f4df39c4c9d822275796a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d80d3d95cd0b4e1299b10e2a7668d720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10fe8317c9744e8694dd679f45189f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil ekstraksi entitas:\n",
      "{'entity': 'I-ORG', 'score': 0.633482, 'index': 1, 'word': 'Nam', 'start': 0, 'end': 3}\n",
      "{'entity': 'I-ORG', 'score': 0.6204568, 'index': 2, 'word': '##any', 'start': 3, 'end': 6}\n",
      "{'entity': 'I-ORG', 'score': 0.602394, 'index': 3, 'word': '##a', 'start': 6, 'end': 7}\n",
      "{'entity': 'I-ORG', 'score': 0.7519675, 'index': 4, 'word': 'ad', 'start': 8, 'end': 10}\n",
      "{'entity': 'I-ORG', 'score': 0.55690503, 'index': 5, 'word': '##ala', 'start': 10, 'end': 13}\n",
      "{'entity': 'I-ORG', 'score': 0.79733634, 'index': 7, 'word': 'And', 'start': 15, 'end': 18}\n",
      "{'entity': 'I-ORG', 'score': 0.47748318, 'index': 8, 'word': '##i', 'start': 18, 'end': 19}\n",
      "{'entity': 'I-LOC', 'score': 0.9574618, 'index': 17, 'word': 'Jakarta', 'start': 39, 'end': 46}\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "from transformers import pipeline\n",
    "\n",
    "# Membuat pipeline untuk Name Entity Recognition (NER) dengan model \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "nlp = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "\n",
    "# Kalimat yang akan diekstraksi entitasnya\n",
    "kalimat = \"Namanya adalah Andi dan dia tinggal di Jakarta.\"\n",
    "\n",
    "# Melakukan ekstraksi entitas menggunakan NER\n",
    "ner_results = nlp(kalimat)\n",
    "\n",
    "# Menampilkan hasil ekstraksi entitas\n",
    "print(\"Hasil ekstraksi entitas:\")\n",
    "for entitas in ner_results:\n",
    "    print(entitas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "nlp = pipeline(\"ner\", model=\"Jean-Baptiste/camembert-ner\", grouped_entities=True)\n",
    "example = \"Her name is Anjela and she lives in Seoul.\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 - Question Answering\n",
    "In this Exercise, use any sentence and a question of choice to extract some information, using \"distilbert-base-cased-distilled-squad\" model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "391e587c719a4bb5aa842558c9139a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f6490e293045aeb9edf1c4fb8f80f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a42b95604047d89577da5b72b96e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cac7f57bddd4565a436da9e1a696aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e49fd40036d64340bad681df571c6306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jawaban: Danau Ontario\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "from transformers import pipeline\n",
    "\n",
    "# Membuat pipeline untuk Question Answering dengan model \"distilbert-base-cased-distilled-squad\"\n",
    "question_answerer = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "# Pertanyaan yang akan dijawab\n",
    "pertanyaan = \"Danau mana yang merupakan salah satu dari Lima Danau Besar Amerika Utara?\"\n",
    "\n",
    "# Konteks teks dimana pertanyaan akan dijawab\n",
    "konteks = \"Danau Ontario adalah salah satu dari Lima Danau Besar Amerika Utara. Danau ini dikelilingi oleh provinsi Ontario, Kanada di utara, barat, dan barat daya, dan di sebelah selatan dan timur oleh negara bagian New York, Amerika Serikat, yang batas airnya, sepanjang perbatasan internasional, bertemu di tengah danau.\"\n",
    "\n",
    "# Melakukan ekstraksi jawaban menggunakan model Question Answering\n",
    "hasil_jawaban = question_answerer(question=pertanyaan, context=konteks)\n",
    "\n",
    "# Menampilkan jawaban\n",
    "print(\"Jawaban:\", hasil_jawaban[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "question_answerer = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
    "question_answerer(\n",
    "    question=\"Which lake is one of the five Great Lakes of North America?\",\n",
    "    context=\"Lake Ontario is one of the five Great Lakes of North America. It is surrounded on the north, west, and southwest by the Canadian province of Ontario, and on the south and east by the U.S. state of New York, whose water boundaries, along the international border, meet in the middle of the lake.\",\n",
    ")\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6 - Text Summarization\n",
    "In this Exercise, use any document/paragraph of choice and summarize it, using \"sshleifer/distilbart-cnn-12-6\" model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/sklearn/utils/validation.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LARGE_SPARSE_SUPPORTED = LooseVersion(scipy_version) >= '0.14.0'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ringkasan:\n",
      " Danau Superior adalah danau paling utara dan barat laut dari Lima Danau Besar Amerika Utara . Danau ini bermuara ke Danau Huron melalui Sungai St. Marys, dawah ke\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "from transformers import pipeline\n",
    "\n",
    "# Membuat pipeline untuk Text Summarization dengan model \"sshleifer/distilbart-cnn-12-6\"\n",
    "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\",  max_length=59)\n",
    "\n",
    "# Dokumen atau paragraf yang akan diringkas\n",
    "dokumen = \"\"\"\n",
    "Danau Superior di Amerika Utara bagian tengah adalah danau air tawar terbesar di dunia menurut luas permukaan dan terbesar ketiga menurut volume, menampung 10% dari air tawar permukaan dunia. Danau Superior adalah danau paling utara dan barat laut dari Lima Danau Besar Amerika Utara, melintasi perbatasan Kanada-Amerika Serikat dengan provinsi Ontario di sebelah utara, dan negara bagian Minnesota di barat laut serta Wisconsin dan Michigan di selatan. Danau ini bermuara ke Danau Huron melalui Sungai St. Marys dan melalui Danau Besar bagian bawah ke Sungai St. Lawrence dan Samudra Atlantik.\n",
    "\"\"\"\n",
    "\n",
    "# Melakukan rangkuman menggunakan model Text Summarization\n",
    "ringkasan = summarizer(dokumen)\n",
    "\n",
    "# Menampilkan ringkasan\n",
    "print(\"Ringkasan:\")\n",
    "print(ringkasan[0]['summary_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\",  max_length=59)\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "Lake Superior in central North America is the largest freshwater lake in the world by surface area and the third-largest by volume, holding 10% of the world's surface fresh water. The northern and westernmost of the Great Lakes of North America, it straddles the Canada–United States border with the province of Ontario to the north, and the states of Minnesota to the northwest and Wisconsin and Michigan to the south. It drains into Lake Huron via St. Marys River and through the lower Great Lakes to the St. Lawrence River and the Atlantic Ocean.\n",
    "\"\"\"\n",
    ")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7 - Translation\n",
    "In this Exercise, use any sentence of choice to translate English to German. The translation model you can use is \"translation_en_to_de\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terjemahan dari bahasa Inggris ke Jerman:\n",
      "New York ist meine Lieblingsstadt\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "from transformers import pipeline\n",
    "\n",
    "# Membuat pipeline untuk terjemahan dari bahasa Inggris ke Jerman dengan model \"translation_en_to_de\"\n",
    "translator = pipeline(\"translation_en_to_de\", model=\"t5-small\")\n",
    "\n",
    "# Kalimat yang akan diterjemahkan dari bahasa Inggris ke Jerman\n",
    "kalimat_inggris = \"New York is my favourite city\"\n",
    "\n",
    "# Melakukan terjemahan\n",
    "terjemahan = translator(kalimat_inggris, max_length=40)\n",
    "\n",
    "# Menampilkan hasil terjemahan\n",
    "print(\"Terjemahan dari bahasa Inggris ke Jerman:\")\n",
    "print(terjemahan[0]['translation_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "translator = pipeline(\"translation_en_to_de\", model=\"t5-small\")\n",
    "print(translator(\"New York is my favourite city\", max_length=40))\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You have completed this guided project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Svitlana Kramar](www.linkedin.com/in/svitlana-kramar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2022-09-20|0.1|Svitlana K.|Created first version|\n",
    "|2022-09-27|0.2|Svitlana K.|Created second version. Added diagrams.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2022 IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
